{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>988</td>\n",
       "      <td>471</td>\n",
       "      <td>1059</td>\n",
       "      <td>1410</td>\n",
       "      <td>848</td>\n",
       "      <td>1742</td>\n",
       "      <td>1269</td>\n",
       "      <td>1662</td>\n",
       "      <td>824</td>\n",
       "      <td>879</td>\n",
       "      <td>...</td>\n",
       "      <td>855</td>\n",
       "      <td>1009</td>\n",
       "      <td>453</td>\n",
       "      <td>1155</td>\n",
       "      <td>974</td>\n",
       "      <td>1145</td>\n",
       "      <td>1029</td>\n",
       "      <td>1217</td>\n",
       "      <td>858</td>\n",
       "      <td>foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>986</td>\n",
       "      <td>530</td>\n",
       "      <td>1291</td>\n",
       "      <td>1693</td>\n",
       "      <td>1016</td>\n",
       "      <td>1386</td>\n",
       "      <td>1349</td>\n",
       "      <td>1586</td>\n",
       "      <td>934</td>\n",
       "      <td>1200</td>\n",
       "      <td>...</td>\n",
       "      <td>728</td>\n",
       "      <td>862</td>\n",
       "      <td>402</td>\n",
       "      <td>832</td>\n",
       "      <td>852</td>\n",
       "      <td>1280</td>\n",
       "      <td>1054</td>\n",
       "      <td>1354</td>\n",
       "      <td>938</td>\n",
       "      <td>foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1132</td>\n",
       "      <td>769</td>\n",
       "      <td>1366</td>\n",
       "      <td>1746</td>\n",
       "      <td>1018</td>\n",
       "      <td>1773</td>\n",
       "      <td>1564</td>\n",
       "      <td>1794</td>\n",
       "      <td>1107</td>\n",
       "      <td>1014</td>\n",
       "      <td>...</td>\n",
       "      <td>705</td>\n",
       "      <td>905</td>\n",
       "      <td>502</td>\n",
       "      <td>932</td>\n",
       "      <td>915</td>\n",
       "      <td>1228</td>\n",
       "      <td>1104</td>\n",
       "      <td>1349</td>\n",
       "      <td>960</td>\n",
       "      <td>left</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>669</td>\n",
       "      <td>435</td>\n",
       "      <td>1183</td>\n",
       "      <td>1370</td>\n",
       "      <td>623</td>\n",
       "      <td>1079</td>\n",
       "      <td>1151</td>\n",
       "      <td>1347</td>\n",
       "      <td>533</td>\n",
       "      <td>896</td>\n",
       "      <td>...</td>\n",
       "      <td>452</td>\n",
       "      <td>517</td>\n",
       "      <td>135</td>\n",
       "      <td>618</td>\n",
       "      <td>647</td>\n",
       "      <td>897</td>\n",
       "      <td>796</td>\n",
       "      <td>1083</td>\n",
       "      <td>792</td>\n",
       "      <td>foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>528</td>\n",
       "      <td>187</td>\n",
       "      <td>969</td>\n",
       "      <td>1208</td>\n",
       "      <td>394</td>\n",
       "      <td>1018</td>\n",
       "      <td>1032</td>\n",
       "      <td>1201</td>\n",
       "      <td>346</td>\n",
       "      <td>692</td>\n",
       "      <td>...</td>\n",
       "      <td>350</td>\n",
       "      <td>254</td>\n",
       "      <td>104</td>\n",
       "      <td>376</td>\n",
       "      <td>596</td>\n",
       "      <td>638</td>\n",
       "      <td>516</td>\n",
       "      <td>619</td>\n",
       "      <td>380</td>\n",
       "      <td>foot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1     2     3     4     5     6     7     8     9  ...   50    51  \\\n",
       "0   988  471  1059  1410   848  1742  1269  1662   824   879  ...  855  1009   \n",
       "1   986  530  1291  1693  1016  1386  1349  1586   934  1200  ...  728   862   \n",
       "2  1132  769  1366  1746  1018  1773  1564  1794  1107  1014  ...  705   905   \n",
       "3   669  435  1183  1370   623  1079  1151  1347   533   896  ...  452   517   \n",
       "4   528  187   969  1208   394  1018  1032  1201   346   692  ...  350   254   \n",
       "\n",
       "    52    53   54    55    56    57   58    59  \n",
       "0  453  1155  974  1145  1029  1217  858  foot  \n",
       "1  402   832  852  1280  1054  1354  938  foot  \n",
       "2  502   932  915  1228  1104  1349  960  left  \n",
       "3  135   618  647   897   796  1083  792  foot  \n",
       "4  104   376  596   638   516   619  380  foot  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"train.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset['59']\n",
    "X = dataset.drop('59', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_x = []\n",
    "for i,r in X.iterrows():\n",
    "    r = (r - r.min())/(r.max() - r.min())\n",
    "    T_x.append(r)\n",
    "\n",
    "T_x = np.array(T_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foot</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      foot  left  right\n",
       "0        1     0      0\n",
       "1        1     0      0\n",
       "2        0     1      0\n",
       "3        1     0      0\n",
       "4        1     0      0\n",
       "...    ...   ...    ...\n",
       "1395     0     1      0\n",
       "1396     0     1      0\n",
       "1397     0     1      0\n",
       "1398     0     0      1\n",
       "1399     0     0      1\n",
       "\n",
       "[1400 rows x 3 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = pd.get_dummies(Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Y[Y==\"left\"]=0\n",
    "#Y[Y==\"right\"]=1\n",
    "#Y[Y==\"foot\"]=2\n",
    "T_y = np.array(Y, dtype=\"int\")\n",
    "T_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_test, Y, Y_test = train_test_split(T_x, T_y, test_size=0.2, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([])\n",
    "    \n",
    "#     model.add(Dense(512, activation=\"relu\"))\n",
    "    \n",
    "#     model.add(Dense(512, activation=\"relu\"))\n",
    "    \n",
    "#     model.add(Dropout(0.1))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "#     model.add(Dense(256, activation=\"relu\"))\n",
    "    \n",
    "#     model.add(Dense(256, activation=\"relu\"))\n",
    "    \n",
    "#     model.add(Dropout(0.1))\n",
    "#     model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    \n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    \n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    \n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    \n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(3, activation=\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.compile(optimizer=Adam(lr=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_139 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "32/32 - 0s - loss: 1.2013 - accuracy: 0.4375 - val_loss: 1.0423 - val_accuracy: 0.5000\n",
      "Epoch 2/200\n",
      "32/32 - 0s - loss: 0.9261 - accuracy: 0.5099 - val_loss: 1.0115 - val_accuracy: 0.5000\n",
      "Epoch 3/200\n",
      "32/32 - 0s - loss: 0.8430 - accuracy: 0.5536 - val_loss: 1.0201 - val_accuracy: 0.4643\n",
      "Epoch 4/200\n",
      "32/32 - 0s - loss: 0.7810 - accuracy: 0.5933 - val_loss: 1.0069 - val_accuracy: 0.4554\n",
      "Epoch 5/200\n",
      "32/32 - 0s - loss: 0.7527 - accuracy: 0.5942 - val_loss: 1.0204 - val_accuracy: 0.4821\n",
      "Epoch 6/200\n",
      "32/32 - 0s - loss: 0.7242 - accuracy: 0.6071 - val_loss: 1.0251 - val_accuracy: 0.4554\n",
      "Epoch 7/200\n",
      "32/32 - 0s - loss: 0.7184 - accuracy: 0.6091 - val_loss: 0.9633 - val_accuracy: 0.5179\n",
      "Epoch 8/200\n",
      "32/32 - 0s - loss: 0.7125 - accuracy: 0.6121 - val_loss: 0.8940 - val_accuracy: 0.4375\n",
      "Epoch 9/200\n",
      "32/32 - 0s - loss: 0.7152 - accuracy: 0.6141 - val_loss: 0.9064 - val_accuracy: 0.5000\n",
      "Epoch 10/200\n",
      "32/32 - 0s - loss: 0.6934 - accuracy: 0.6171 - val_loss: 0.9300 - val_accuracy: 0.4732\n",
      "Epoch 11/200\n",
      "32/32 - 0s - loss: 0.6693 - accuracy: 0.6409 - val_loss: 0.9575 - val_accuracy: 0.4911\n",
      "Epoch 12/200\n",
      "32/32 - 0s - loss: 0.6513 - accuracy: 0.6458 - val_loss: 0.8885 - val_accuracy: 0.4821\n",
      "Epoch 13/200\n",
      "32/32 - 0s - loss: 0.6540 - accuracy: 0.6617 - val_loss: 0.8679 - val_accuracy: 0.5000\n",
      "Epoch 14/200\n",
      "32/32 - 0s - loss: 0.6138 - accuracy: 0.6984 - val_loss: 0.8657 - val_accuracy: 0.5536\n",
      "Epoch 15/200\n",
      "32/32 - 0s - loss: 0.6168 - accuracy: 0.6766 - val_loss: 0.8892 - val_accuracy: 0.4911\n",
      "Epoch 16/200\n",
      "32/32 - 0s - loss: 0.5983 - accuracy: 0.6845 - val_loss: 0.8806 - val_accuracy: 0.5536\n",
      "Epoch 17/200\n",
      "32/32 - 0s - loss: 0.6185 - accuracy: 0.6875 - val_loss: 1.2684 - val_accuracy: 0.4911\n",
      "Epoch 18/200\n",
      "32/32 - 0s - loss: 0.5833 - accuracy: 0.7123 - val_loss: 1.3145 - val_accuracy: 0.4732\n",
      "Epoch 19/200\n",
      "32/32 - 0s - loss: 0.5939 - accuracy: 0.6935 - val_loss: 1.3978 - val_accuracy: 0.4554\n",
      "Epoch 20/200\n",
      "32/32 - 0s - loss: 0.5896 - accuracy: 0.6905 - val_loss: 0.9240 - val_accuracy: 0.5179\n",
      "Epoch 21/200\n",
      "32/32 - 0s - loss: 0.5433 - accuracy: 0.7173 - val_loss: 1.1242 - val_accuracy: 0.4821\n",
      "Epoch 22/200\n",
      "32/32 - 0s - loss: 0.5567 - accuracy: 0.7173 - val_loss: 0.9145 - val_accuracy: 0.5625\n",
      "Epoch 23/200\n",
      "32/32 - 0s - loss: 0.5269 - accuracy: 0.7361 - val_loss: 0.9468 - val_accuracy: 0.5179\n",
      "Epoch 24/200\n",
      "32/32 - 0s - loss: 0.5173 - accuracy: 0.7431 - val_loss: 1.1148 - val_accuracy: 0.5179\n",
      "Epoch 25/200\n",
      "32/32 - 0s - loss: 0.5118 - accuracy: 0.7540 - val_loss: 1.1282 - val_accuracy: 0.5982\n",
      "Epoch 26/200\n",
      "32/32 - 0s - loss: 0.5168 - accuracy: 0.7470 - val_loss: 0.9091 - val_accuracy: 0.5000\n",
      "Epoch 27/200\n",
      "32/32 - 0s - loss: 0.4900 - accuracy: 0.7688 - val_loss: 1.7244 - val_accuracy: 0.4643\n",
      "Epoch 28/200\n",
      "32/32 - 0s - loss: 0.5899 - accuracy: 0.6984 - val_loss: 1.0530 - val_accuracy: 0.4554\n",
      "Epoch 29/200\n",
      "32/32 - 0s - loss: 0.5312 - accuracy: 0.7431 - val_loss: 1.0447 - val_accuracy: 0.5268\n",
      "Epoch 30/200\n",
      "32/32 - 0s - loss: 0.4871 - accuracy: 0.7560 - val_loss: 1.1404 - val_accuracy: 0.5089\n",
      "Epoch 31/200\n",
      "32/32 - 0s - loss: 0.5098 - accuracy: 0.7550 - val_loss: 1.0851 - val_accuracy: 0.4286\n",
      "Epoch 32/200\n",
      "32/32 - 0s - loss: 0.5293 - accuracy: 0.7421 - val_loss: 1.0299 - val_accuracy: 0.5000\n",
      "Epoch 33/200\n",
      "32/32 - 0s - loss: 0.5530 - accuracy: 0.7262 - val_loss: 1.0382 - val_accuracy: 0.6339\n",
      "Epoch 34/200\n",
      "32/32 - 0s - loss: 0.4684 - accuracy: 0.7768 - val_loss: 1.0227 - val_accuracy: 0.4196\n",
      "Epoch 35/200\n",
      "32/32 - 0s - loss: 0.4791 - accuracy: 0.7569 - val_loss: 1.2623 - val_accuracy: 0.4286\n",
      "Epoch 36/200\n",
      "32/32 - 0s - loss: 0.4799 - accuracy: 0.7688 - val_loss: 0.8455 - val_accuracy: 0.5357\n",
      "Epoch 37/200\n",
      "32/32 - 0s - loss: 0.4979 - accuracy: 0.7599 - val_loss: 1.2198 - val_accuracy: 0.4554\n",
      "Epoch 38/200\n",
      "32/32 - 0s - loss: 0.4970 - accuracy: 0.7788 - val_loss: 1.8150 - val_accuracy: 0.4196\n",
      "Epoch 39/200\n",
      "32/32 - 0s - loss: 0.4557 - accuracy: 0.7857 - val_loss: 1.3222 - val_accuracy: 0.4375\n",
      "Epoch 40/200\n",
      "32/32 - 0s - loss: 0.5035 - accuracy: 0.7788 - val_loss: 1.2676 - val_accuracy: 0.4911\n",
      "Epoch 41/200\n",
      "32/32 - 0s - loss: 0.4507 - accuracy: 0.7946 - val_loss: 1.2724 - val_accuracy: 0.5000\n",
      "Epoch 42/200\n",
      "32/32 - 0s - loss: 0.4454 - accuracy: 0.8016 - val_loss: 1.1111 - val_accuracy: 0.5268\n",
      "Epoch 43/200\n",
      "32/32 - 0s - loss: 0.4368 - accuracy: 0.8065 - val_loss: 1.1895 - val_accuracy: 0.5714\n",
      "Epoch 44/200\n",
      "32/32 - 0s - loss: 0.3973 - accuracy: 0.8264 - val_loss: 1.0607 - val_accuracy: 0.6071\n",
      "Epoch 45/200\n",
      "32/32 - 0s - loss: 0.3997 - accuracy: 0.7966 - val_loss: 1.3846 - val_accuracy: 0.5000\n",
      "Epoch 46/200\n",
      "32/32 - 0s - loss: 0.4564 - accuracy: 0.7768 - val_loss: 1.3470 - val_accuracy: 0.5000\n",
      "Epoch 47/200\n",
      "32/32 - 0s - loss: 0.4344 - accuracy: 0.8026 - val_loss: 0.9721 - val_accuracy: 0.4732\n",
      "Epoch 48/200\n",
      "32/32 - 0s - loss: 0.4045 - accuracy: 0.8085 - val_loss: 1.1017 - val_accuracy: 0.4911\n",
      "Epoch 49/200\n",
      "32/32 - 0s - loss: 0.4218 - accuracy: 0.8006 - val_loss: 1.4180 - val_accuracy: 0.4732\n",
      "Epoch 50/200\n",
      "32/32 - 0s - loss: 0.4318 - accuracy: 0.8085 - val_loss: 1.4779 - val_accuracy: 0.4821\n",
      "Epoch 51/200\n",
      "32/32 - 0s - loss: 0.3993 - accuracy: 0.8125 - val_loss: 1.4055 - val_accuracy: 0.5179\n",
      "Epoch 52/200\n",
      "32/32 - 0s - loss: 0.3834 - accuracy: 0.8185 - val_loss: 1.2043 - val_accuracy: 0.5268\n",
      "Epoch 53/200\n",
      "32/32 - 0s - loss: 0.4726 - accuracy: 0.7738 - val_loss: 1.8862 - val_accuracy: 0.4107\n",
      "Epoch 54/200\n",
      "32/32 - 0s - loss: 0.4281 - accuracy: 0.8046 - val_loss: 1.0531 - val_accuracy: 0.5446\n",
      "Epoch 55/200\n",
      "32/32 - 0s - loss: 0.4452 - accuracy: 0.7877 - val_loss: 1.3062 - val_accuracy: 0.4911\n",
      "Epoch 56/200\n",
      "32/32 - 0s - loss: 0.3731 - accuracy: 0.8423 - val_loss: 1.1422 - val_accuracy: 0.5536\n",
      "Epoch 57/200\n",
      "32/32 - 0s - loss: 0.3862 - accuracy: 0.8056 - val_loss: 1.3431 - val_accuracy: 0.4911\n",
      "Epoch 58/200\n",
      "32/32 - 0s - loss: 0.3510 - accuracy: 0.8492 - val_loss: 1.0131 - val_accuracy: 0.5893\n",
      "Epoch 59/200\n",
      "32/32 - 0s - loss: 0.3457 - accuracy: 0.8552 - val_loss: 1.0913 - val_accuracy: 0.4821\n",
      "Epoch 60/200\n",
      "32/32 - 0s - loss: 0.3801 - accuracy: 0.8423 - val_loss: 1.3929 - val_accuracy: 0.4643\n",
      "Epoch 61/200\n",
      "32/32 - 0s - loss: 0.3623 - accuracy: 0.8244 - val_loss: 1.4540 - val_accuracy: 0.4643\n",
      "Epoch 62/200\n",
      "32/32 - 0s - loss: 0.3509 - accuracy: 0.8343 - val_loss: 1.2193 - val_accuracy: 0.4643\n",
      "Epoch 63/200\n",
      "32/32 - 0s - loss: 0.2911 - accuracy: 0.8889 - val_loss: 1.3007 - val_accuracy: 0.5536\n",
      "Epoch 64/200\n",
      "32/32 - 0s - loss: 0.2778 - accuracy: 0.8839 - val_loss: 1.9375 - val_accuracy: 0.4375\n",
      "Epoch 65/200\n",
      "32/32 - 0s - loss: 0.3180 - accuracy: 0.8641 - val_loss: 1.5915 - val_accuracy: 0.4821\n",
      "Epoch 66/200\n",
      "32/32 - 0s - loss: 0.3951 - accuracy: 0.8304 - val_loss: 1.5421 - val_accuracy: 0.4821\n",
      "Epoch 67/200\n",
      "32/32 - 0s - loss: 0.3222 - accuracy: 0.8621 - val_loss: 1.7107 - val_accuracy: 0.5000\n",
      "Epoch 68/200\n",
      "32/32 - 0s - loss: 0.3256 - accuracy: 0.8631 - val_loss: 1.7331 - val_accuracy: 0.5089\n",
      "Epoch 69/200\n",
      "32/32 - 0s - loss: 0.3091 - accuracy: 0.8710 - val_loss: 1.6667 - val_accuracy: 0.4554\n",
      "Epoch 70/200\n",
      "32/32 - 0s - loss: 0.2809 - accuracy: 0.8710 - val_loss: 1.4514 - val_accuracy: 0.5000\n",
      "Epoch 71/200\n",
      "32/32 - 0s - loss: 0.3257 - accuracy: 0.8562 - val_loss: 3.4310 - val_accuracy: 0.4196\n",
      "Epoch 72/200\n",
      "32/32 - 0s - loss: 0.2986 - accuracy: 0.8770 - val_loss: 2.0279 - val_accuracy: 0.4464\n",
      "Epoch 73/200\n",
      "32/32 - 0s - loss: 0.3150 - accuracy: 0.8671 - val_loss: 1.3396 - val_accuracy: 0.4643\n",
      "Epoch 74/200\n",
      "32/32 - 0s - loss: 0.2966 - accuracy: 0.8730 - val_loss: 1.5101 - val_accuracy: 0.5089\n",
      "Epoch 75/200\n",
      "32/32 - 0s - loss: 0.2623 - accuracy: 0.8938 - val_loss: 1.6389 - val_accuracy: 0.5089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/200\n",
      "32/32 - 0s - loss: 0.2369 - accuracy: 0.8988 - val_loss: 1.7772 - val_accuracy: 0.5089\n",
      "Epoch 77/200\n",
      "32/32 - 0s - loss: 0.2630 - accuracy: 0.8958 - val_loss: 1.7150 - val_accuracy: 0.5179\n",
      "Epoch 78/200\n",
      "32/32 - 0s - loss: 0.2951 - accuracy: 0.8760 - val_loss: 1.4510 - val_accuracy: 0.4821\n",
      "Epoch 79/200\n",
      "32/32 - 0s - loss: 0.2729 - accuracy: 0.8948 - val_loss: 1.8150 - val_accuracy: 0.4375\n",
      "Epoch 80/200\n",
      "32/32 - 0s - loss: 0.2503 - accuracy: 0.9038 - val_loss: 1.6552 - val_accuracy: 0.4821\n",
      "Epoch 81/200\n",
      "32/32 - 0s - loss: 0.3224 - accuracy: 0.8562 - val_loss: 1.6076 - val_accuracy: 0.5089\n",
      "Epoch 82/200\n",
      "32/32 - 0s - loss: 0.3115 - accuracy: 0.8700 - val_loss: 1.5428 - val_accuracy: 0.4821\n",
      "Epoch 83/200\n",
      "32/32 - 0s - loss: 0.2391 - accuracy: 0.8909 - val_loss: 1.5445 - val_accuracy: 0.5268\n",
      "Epoch 84/200\n",
      "32/32 - 0s - loss: 0.2324 - accuracy: 0.9008 - val_loss: 1.6700 - val_accuracy: 0.5089\n",
      "Epoch 85/200\n",
      "32/32 - 0s - loss: 0.3027 - accuracy: 0.8790 - val_loss: 1.6710 - val_accuracy: 0.4821\n",
      "Epoch 86/200\n",
      "32/32 - 0s - loss: 0.2632 - accuracy: 0.8998 - val_loss: 1.7370 - val_accuracy: 0.4464\n",
      "Epoch 87/200\n",
      "32/32 - 0s - loss: 0.2213 - accuracy: 0.9157 - val_loss: 1.6025 - val_accuracy: 0.5268\n",
      "Epoch 88/200\n",
      "32/32 - 0s - loss: 0.2873 - accuracy: 0.8770 - val_loss: 2.0369 - val_accuracy: 0.4464\n",
      "Epoch 89/200\n",
      "32/32 - 0s - loss: 0.2738 - accuracy: 0.8819 - val_loss: 1.4521 - val_accuracy: 0.5536\n",
      "Epoch 90/200\n",
      "32/32 - 0s - loss: 0.2291 - accuracy: 0.9058 - val_loss: 1.7364 - val_accuracy: 0.4821\n",
      "Epoch 91/200\n",
      "32/32 - 0s - loss: 0.2270 - accuracy: 0.9028 - val_loss: 1.7325 - val_accuracy: 0.4732\n",
      "Epoch 92/200\n",
      "32/32 - 0s - loss: 0.1995 - accuracy: 0.9167 - val_loss: 1.7217 - val_accuracy: 0.5089\n",
      "Epoch 93/200\n",
      "32/32 - 0s - loss: 0.2410 - accuracy: 0.8988 - val_loss: 2.1990 - val_accuracy: 0.4732\n",
      "Epoch 94/200\n",
      "32/32 - 0s - loss: 0.2287 - accuracy: 0.9048 - val_loss: 1.6132 - val_accuracy: 0.5179\n",
      "Epoch 95/200\n",
      "32/32 - 0s - loss: 0.2399 - accuracy: 0.9058 - val_loss: 1.7220 - val_accuracy: 0.4821\n",
      "Epoch 96/200\n",
      "32/32 - 0s - loss: 0.2330 - accuracy: 0.9097 - val_loss: 1.6267 - val_accuracy: 0.5089\n",
      "Epoch 97/200\n",
      "32/32 - 0s - loss: 0.1908 - accuracy: 0.9256 - val_loss: 1.5448 - val_accuracy: 0.5000\n",
      "Epoch 98/200\n",
      "32/32 - 0s - loss: 0.1544 - accuracy: 0.9375 - val_loss: 1.8551 - val_accuracy: 0.5536\n",
      "Epoch 99/200\n",
      "32/32 - 0s - loss: 0.1982 - accuracy: 0.9226 - val_loss: 2.2167 - val_accuracy: 0.5179\n",
      "Epoch 100/200\n",
      "32/32 - 0s - loss: 0.1947 - accuracy: 0.9246 - val_loss: 1.6644 - val_accuracy: 0.5357\n",
      "Epoch 101/200\n",
      "32/32 - 0s - loss: 0.1620 - accuracy: 0.9286 - val_loss: 1.9017 - val_accuracy: 0.5179\n",
      "Epoch 102/200\n",
      "32/32 - 0s - loss: 0.1521 - accuracy: 0.9415 - val_loss: 2.0208 - val_accuracy: 0.4911\n",
      "Epoch 103/200\n",
      "32/32 - 0s - loss: 0.1952 - accuracy: 0.9177 - val_loss: 1.9393 - val_accuracy: 0.5357\n",
      "Epoch 104/200\n",
      "32/32 - 0s - loss: 0.2218 - accuracy: 0.9236 - val_loss: 1.9962 - val_accuracy: 0.5357\n",
      "Epoch 105/200\n",
      "32/32 - 0s - loss: 0.2647 - accuracy: 0.8849 - val_loss: 1.7969 - val_accuracy: 0.5714\n",
      "Epoch 106/200\n",
      "32/32 - 0s - loss: 0.2125 - accuracy: 0.9127 - val_loss: 1.5992 - val_accuracy: 0.5536\n",
      "Epoch 107/200\n",
      "32/32 - 0s - loss: 0.1872 - accuracy: 0.9236 - val_loss: 1.8533 - val_accuracy: 0.5089\n",
      "Epoch 108/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-de11ff42ab8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\acer\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acer\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    853\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acer\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \"\"\"\n\u001b[0;32m    388\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acer\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_process_logs\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;34m\"\"\"Turns tensors into numpy arrays or Python scalars.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acer\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acer\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 617\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acer\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 617\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acer\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    517\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acer\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \"\"\"\n\u001b[0;32m    960\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 961\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    962\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\acer\\anaconda3\\envs\\gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    925\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 927\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    928\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = my_model.fit(X, Y, validation_split=0.1, shuffle = True, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 3ms/step - loss: 2.1460 - accuracy: 0.4571\n"
     ]
    }
   ],
   "source": [
    "loss, acc = my_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
